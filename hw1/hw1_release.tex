\documentclass[submit]{../harvardml}
\usepackage{../common}

\course{CS1810-S26}
\assignment{Homework \#1}
\duedate{February 13, 2026 at 11:59 PM}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{framed}
\usepackage{float}
\usepackage{ifthen}
\usepackage{bm}


\usepackage[mmddyyyy,hhmmss]{datetime}



\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

 \DeclareMathOperator*{\limover}{\overline{lim}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Solution environment
\usepackage{xcolor}
\newenvironment{solution}{
    \vspace{2mm}
    \color{blue}\noindent\textbf{Solution}:
}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{center}
  {\Large Regression}
\end{center}

\subsection*{Introduction}

This homework is on different three different forms of regression:
nearest neighbors regression, kernelized regression, and linear
regression.  We will discuss implementation and examine their
tradeoffs by implementing them on the same dataset, which consists of
temperature over the past 800,000 years taken from ice core samples.

The folder \verb|data| contains the data you will use for this
problem. There are two files:
\begin{itemize}
  \item \verb|earth_temperature_sampled_train.csv|
  \item \verb|earth_temperature_sampled_test.csv|
\end{itemize}

Each has two columns.  The first column is the age of the ice core
sample.  The second column is the approximate difference in a year's temperature (K)
from the average temperature of the 1,000 years preceding it. The temperatures were retrieved from ice cores in
Antarctica (Jouzel et al. 2007)\footnote{Retrieved from
  \url{https://www.ncei.noaa.gov/pub/data/paleo/icecore/antarctica/epica_domec/edc3deuttemp2007.txt}

  Jouzel, J., Masson-Delmotte, V., Cattani, O., Dreyfus, G., Falourd,
  S., Hoffmann, G., … Wolff, E. W. (2007). Orbital and Millennial
  Antarctic Climate Variability over the Past 800,000 Years.
  \emph{Science, 317}(5839), 793–796. doi:10.1126/science.1141038}.

The following is a snippet of the data file:

\begin{csv}
  # Age, Temperature
  399946,0.51
  409980,1.57
\end{csv}

\noindent And this is a visualization of the full dataset:
\begin{center}
  \includegraphics[width=.8\textwidth]{img_input/sample_graph}
\end{center}
\noindent


\textbf{Due to the large magnitude of the years, we will work in terms
  of thousands of years BCE in these problems.} This is taken care of
for you in the provided notebook.






\subsection*{Resources and Submission Instructions}

% The course textbook is not fully updated to the spring 2026 rendition of CS 1810, but if you want to take a look, it may still be helpful to see Sections 2.1-2.7 of the \href{https://github.com/harvard-ml-courses/cs181-textbook/blob/master/Textbook.pdf}{cs181-textbook notes}.

% We also encourage you to first read the
% \href{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf}{Bishop textbook}, particularly: Section 2.3 (Properties of Gaussian Distributions), Section 3.1 (Linear Basis Regression), and Section 3.3 (Bayesian Linear Regression). (Note that our notation is slightly different but the underlying mathematics remains the same!).

Please type your solutions after the corresponding problems using this \LaTeX\ template, and start each main problem on a new page.

Please submit the writeup PDF to the Gradescope assignment `HW1'. Remember to assign pages for each question.  \textbf{You must include any plots in your writeup PDF. }. Please submit your \LaTeX file and code files to the Gradescope assignment `HW1 - Supplemental.' The supplemental files will only be checked in special cases, e.g. honor code issues, etc. Your files should be named in the same way as we provide them in the repository, e.g. \texttt{hw1.pdf}, etc.

If you find that you are having trouble with the first couple problems, it may be helpful to refer to Section 0 notes and review some linear algebra and matrix calculus. 

\newpage 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[kNN and Kernels, 35pts]

You will now implement two non-parametric regressions to model temperatures over time.  
% For this problem, you will use the \textbf{same dataset as in Problem 1}.

\vspace{0.5cm}
\noindent\emph{Make sure to include all required plots in your PDF. Passing all test cases does not guarantee that your solution is correct, and we encourage you to write your own. }

\begin{enumerate}
\item 
 Recall that kNN uses a predictor of the form
\[
  f(x^*) = \frac{1}{k} \sum_n y_n \mathbb{I}(x_n \texttt{ is one of k-closest to } x^*),
\]
where $\mathbb{I}$ is an indicator variable. 
\begin{enumerate}

  \item The kNN implementation \textbf{has been provided for you} in the notebook. Run the cells to plot the results for $k=\{1, 3, N-1\}$, where $N$ is the size of the dataset. Describe how the fits change with $k$. Please include your plot in your solution PDF.

  \item Now, we will evaluate the quality of each model \emph{quantitatively} by computing the error on the provided test set. Write Python code to compute the test MSE for each value of $k$.  Report the values here. Which solution has the lowest MSE? 
  
\end{enumerate}

\item \textit{Kernel-based regression} techniques are another form of non-parametric regression. Consider a kernel-based
regressor of the form 
\begin{equation*}
  f_\tau(x^*) = \cfrac{\sum_{n} K_\tau(x_n,x^*) y_n}{\sum_n K_\tau(x_n, x^*)}
\end{equation*}
where $\mathcal{D}_\texttt{train} = \{(x_n,y_n)\}_{n = 1} ^N$ are the
training data points, and $x^*$ is the point for which you want to
make the prediction.  The kernel $K_\tau(x,x')$ is a function that
defines the similarity between two inputs $x$ and $x'$. A popular
choice of kernel is a function that decays as the distance between the
two points increases, such as
\begin{equation*}
  K_\tau(x,x') = \exp\left(-\frac{(x-x')^2}{\tau}\right)
\end{equation*}

where $\tau$ represents the square of the lengthscale (a scalar value that
dictates how quickly the kernel decays).  


\begin{enumerate}
    
  \item First, implement the \texttt{kernel\_regressor} function in the notebook, and plot your model for years in the range $800,000$ BC to $400,000$ BC at $1000$ year intervals for the following three values of $\tau$: $1, 50, 2500$. Since we're working in terms of thousands of years, this means you should plot $(x, f_\tau(x))$ for $x = 400, 401, \dots, 800$. \textbf{In no more than 10 lines}, describe how the fits change with $\tau$. Please include your plot in your solution PDF.

  \item Denote the test set as $\mathcal{D}_\texttt{test} = \{(x'_m, y'_m)\}_{m = 1} ^M$.  Write down the expression for the MSE of $f_\tau$ over the test set as a function of the training set and test set. Your answer may include $\{(x'_m, y'_m)\}_{m = 1} ^M$, $\{(x_n, y_n)\}_{n = 1} ^N$, and $K_\tau$, but not $f_\tau$.

    \item Compute the MSE on the provided test set for the three values of $\tau$.  Report the values here. Which model yields the lowest MSE? Conceptually, why is this the case? Why would choosing $\tau$ based on $\mathcal{D}_\texttt{train}$ rather than $\mathcal{D}_\texttt{test}$ be a bad idea? 

  \item Describe the time and space complexity of both kernelized regression and kNN with respect to the size of the training set $N$.  How, if at all, does the size of the model---everything that needs to be stored to make predictions---change with the size of the training set $N$?  How, if at all, do the number of computations required to make a prediction for some input $x^*$ change with the size of the training set $N$?.
  

  \item  What is the exact form of $\lim_{\tau \to 0 }f_\tau(x^*)$?
  \end{enumerate}
\end{enumerate}
\end{problem}

\newpage

\begin{solution}
	\begin{enumerate}
	    \item \begin{enumerate}
	        \item From the plot, it is clear the lower $k$, the higher variance and lower bias. The higher $k$, the higher bias and lower variance. When $k=1$, the plot follows exactly each training point, which is the highest variance. When we increase $k = 3$, variance slightly decreases. When $k = N-1$, we get the highest bias and smallest variance, which is just a flat curve with the prediction equal to the average of all points.\\
            \includegraphics[]{hw1/img_output/p1.1a.png}
            \item $k = 1: 1.74, k=3: 3.89, k = N -1: 9.53$, so $k = 1$ has the lowest MSE. This is because the training and testing data set are sampled from the same time series of temperature measurements, so the nearest neighbor will give us a lower MSE then averaging more neighbors.
	    \end{enumerate}
        \item \begin{enumerate}
            \item In general, the fit changes with $\tau$ similarly to $k$ in kNN: the lower $\tau$, the higher variance and lower bias. The higher $\tau$, the higher bias and lower variance. This goes in line with the definition of $\tau$, dictating how quickly the kernel decays; when low, it decays very quickly and only captures local detail, and as it increases, it gives more weight to a wider portion of neighbors. Also, the higher $\tau$, the smoother the curve, which also comes from the decay being slower.\\
            \includegraphics[]{hw1/img_output/p1.2a.png}
            \item $MSE = \cfrac{1}{M}\sum_{m}(\cfrac{\sum_{n} K_\tau(x_n,x'_m) y_n}{\sum_n K_\tau(x_n, x'_m)} - y'_m)^2$.
            \item Test MSE for $\tau = 1: 1.95, \tau = 50: 1.86, \tau = 2500: 8.33$, so $\tau = 50$ gives us lowest MSE. Conceptually, $\tau = 1$ gives us a very similar curve to that of $k = 1$ with kNN, and so the predictions are mostly only relying on the nearest neighbor. With $\tau = 50$, the slightly slower decay still weighs nearest points heavily, but is also able to capture the general trend of the curve better since it's less sensitive to individual noise, which is why it has a lower MSE. Then, $\tau = 2500$ has a very high bias, and gives us too flat a curve close to the average of all points, similarly to kNN of all points.
            \item Both methods require storing the entire training set $N$ to make predictions, either to determine which are the nearest $k$ neighbors, or using kernel weights for each training point, so space complexity is $O(N)$.\\
            Time complexity is also $O(N)$ for both methods predictions, since kNN has to compute all distances from $x^*$ to all training points to get nearest $k$ neighbors, and kernelized regression needs kernel weights between $x^*$ and each training point.
            \item The term $K_\tau(x,x') = \exp\left(-\frac{(x-x')^2}{\tau}\right)$ will be extremely small for each point, and the nearest point has the smallest $(x^* - x_i)^2$, so its weight decays slowest relative to all other points, and so after normalization, it will have a weight close to 1 and all other points close to 0. We will thus have $\lim_{\tau \to 0 }f_\tau(x^*)=y_j$, $j = arg\ min_i (x^* - x_i)^2$, which is kNN with $k = 1$, which also confirms reasoning in part (c) for $\tau = 1$.
        \end{enumerate} 
        
	\end{enumerate}
\end{solution}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Geometric Least Squares, 20pts]
    Linear regression can be understood using geometric intuition in $\mathbb{R}^n$. The design matrix $\mathbf X \in \mathbb{R}^{N\times D}$, with $N > D$, spans a subspace $C(\mathbf X)$, the column space of $\mathbf X$ (referred to in lecture as column span) which is a subspace of $\mathbb{R}^N$. If you wish to review the concept of a column space, consider visiting Section 0 notes. \\
    
    \noindent Fitting by linear regression, sometimes called \textit{ordinary least-squares} (OLS), is just projecting the observation vector $\mathbf y \in \mathbb{R}^N$ orthogonally onto that subspace. Lecture 2 slides provide a good graphic to visualize this, see the slide titled ``Geometric Interpretation.'' From lecture, we also learned that $\hat {\mathbf y}$ lives in $C(\mathbf X)$ and the residual $\mathbf r = {\mathbf y} - \hat {\mathbf y}$ lives in the orthogonal complement.

    \begin{enumerate}
    \item Let $\mathbf X \in \mathbb{R}^{N\times D}$ have full column rank $d$ and $\mathbf y \in \mathbb{R}^N$. Let the OLS estimator be $\mathbf w^*=(\mathbf X^\top \mathbf X)^{-1} X^\top \mathbf y$ and the fitted vector $\mathbf{\hat{y}} = \mathbf X \mathbf w^*$. Prove that $\hat {\mathbf y}$ is the \textit{orthogonal projection} of $y$ onto $C(\mathbf X)$. In other words, show that $\hat {\mathbf y} \in C(\mathbf X)$ and $\mathbf y - \hat {\mathbf y}$ is orthogonal to $C(\mathbf X)$. \textit{Hint: To show orthogonality, look at the gradient of $\mathcal L$, the loss, with respect to $\mathbf w$}.

    \item Prove that among all vectors in $C(\mathbf X)$, the fitted vector $\hat {\mathbf y}$ minimizes the Euclidean distance to $\mathbf y$. In other words, that for every vector $\mathbf v \in C(\mathbf X)$:
    \begin{equation*}
        \|\mathbf y - \hat {\mathbf y}\|_2^2 \leq \|\mathbf y - \mathbf v\|_2^2
    \end{equation*}
    Looking back at lecture, this is the formal proof of the phenomenon discussed in the image. \textit{Hint: For two vectors, $\mathbf v,\mathbf w$, if $\mathbf v$ is orthogonal to $\mathbf w$, denoted as $\mathbf v \perp \mathbf w$, then $\|\mathbf v-\mathbf w\|^2_2 = \|\mathbf v\|_2^2+\|\mathbf w\|^2_2$ (Pythagorean theorem).}

    \item In lecture, we defined the projection matrix, $\mathbf P = \mathbf X(\mathbf X^\top \mathbf X)^{-1} \mathbf X^\top$, which projects onto the subspace $C(\boldX)$. The matrix $\mathbf P$ is often called the \textit{hat matrix} because it maps $\mathbf y$ to its fitted values $\hat {\mathbf y} = \mathbf P \mathbf y$, i.e., it ``puts a hat" on $\mathbf y$. Prove the following properties of $\mathbf P$:
    \begin{itemize}
        \item Symmetry: $\mathbf P^\top = \mathbf P$
        \item Idempotence: $\mathbf P^2 = \mathbf P$
        \item Rank and Trace: $\mathrm{rank}(\mathbf P) = d$ and $\mathrm{trace}(\mathbf P) = d$.
    \end{itemize}
    % Also, provide geometric interpretation of the first two properties. 
    \textit{Hint: You may use the fact that any idempotent matrix has equal rank and trace. You do not need to prove this, but it may be helpful to think about why this is true.}
    
    \item Suppose you fit your model as in Problem 5.1. You observe that the \textbf{residual plot} exhibits a clear parabolic (U-shaped) pattern rather than random scatter around zero (as seen in lecture). Give a geometric interpretation of this phenomenon in terms of projection onto the column space of the design matrix. Also, explain how adding a quadratic basis function affects the geometry of the regression problem and the residuals.
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.7]{img_input/residual_plot.png}
            \caption{An example residual plot with the input variable $x$ on the horizontal axis and residuals $y-\hat{y}$ on the vertical axis.}

          \end{figure}
    \end{enumerate}
\end{problem}

\newpage

\begin{solution}
	\begin{enumerate}
	    \item We have $\hat y = Xw^*$. Since $w^*$ is just a vector of scalars, $\hat y$ is a linear combination of columns of $X$, thus $\hat y \in C(X)$.\\
        We know $L = \frac{1}{n}||y - Xw||^2_2$. We will take the gradient $\frac{d}{dw}L = -\frac{2}{n}X^T(y - Xw)$. When $w = w^*$, $L$ is minimized, and so its gradient is 0, so $0 = -\frac{2}{n}X^T(y - Xw^*)$. Dropping the constant and plugging in $\hat y$, we get $0 = X^T(y - \hat y)$, which is only true if $y - \hat y$ is orthogonal to $C(X)$, thus completing our proof.
        \item for every vector $v$, we can rewrite $||y - v||_2^2 = ||(y - \hat y ) + (\hat y - v)||_2^2$. We know that $y - \hat y $ is orthogonal to $C(X)$, and $\hat y - v \in C(X)$ since both $y$ and $v$ are $\in C(X)$. Thus, $y - \hat y $ is orthogonal to $\hat y - v$. Now, we can apply the Pythagorean theorem, and get $||y - v||^2_2 = ||y - \hat y||_2^2 + ||\hat y - v||_2^2$. Since $||\hat y - v||_2^2$ $\geq 0$, we showed that $||y - \hat y||_2^2\leq||y - v||_2^2$, completing our proof.
        \item For any matrix $X$, $X^TX$ is symmetric. Now, \[P^T = (\mathbf X(\mathbf X^\top \mathbf X)^{-1} \mathbf X^\top)^T,\] \[P^T = (X^T)^T((X^T X)^{-1})^T X^T,\] \[P^T = X((X^T X)^T)^{-1} X^T,\] \[P^T = X(X^T X)^{-1} X^T = P,\] proving $P$ is symmetric.\\
        \[P^2 = [X(X^T X)^{-1} X^T][X(X^T X)^{-1} X^T],\] \[P^2 = X(X^T X)^{-1} X^TX(X^T X)^{-1} X^T,\] \[P^2 = X(X^T X)^{-1} X^T = P,\] since $X^TX(X^T X)^{-1}$ cancels out, proving idempotence.\\
        Lastly, using $trace(ABC) = trace(CAB),$ \[trace(P) = trace[X(X^T X)^{-1} X^T] = trace[X^TX(X^T X)^{-1}] = trace(Id) = d, \] and given idempotence, $rank(P) = trace(P) = d$, completing our proof.
        \item We know that $r = y - \hat y$ is orthogonal to $C(X)$. If $r$ exhibits a parabolic pattern, we could capture that as $r = cx^2 + \epsilon$, $\epsilon \sim N(0, \sigma^2)$. This must mean that $C(X)$ does not span $x^2$. If we added a quadratic basis function, then $C(X)$ would span $x^2$, and since $r$ is orthogonal to $C(X)$, it can't contain an $x^2$ component, leaving $r = \epsilon$, which would just be random noise, getting rid of the parabolic pattern.
	\end{enumerate}
\end{solution}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Basis Regression, 30pts]
    We now implement some linear regression models for the temperature. If we just directly use the data as given to us, we would only have a one dimensional input to our model, the year.  To create a more expressive linear model, we will introduce basis functions.
    
    \vspace{1em}
    
    \noindent\emph{Make sure to include all required plots in your PDF.}
    
    \begin{enumerate}
        \item We will first implement the four basis regressions below. Note that we introduce an addition transform $f$ (already into the provided notebook) to address concerns about numerical instabilities.
        
        \begin{enumerate}
            \item $\phi_j(x)= f(x)^j$ for $j=1,\ldots, 9$. $f(x) = \frac{x}{1.81 \cdot 10^{2}}.$
          
            \item $\phi_j(x) = \exp\left\{-\cfrac{(f(x)-\mu_j)^2}{5}\right\}$ for $\mu_j=\frac{j + 7}{8}$ with $j=1,\ldots, 9$. $f(x) = \frac{x}{4.00 \cdot 10^{2}}.$
          
            \item $\phi_j(x) =  \cos(f(x) / j)$ for $j=1, \ldots, 9$. $f(x) = \frac{x}{1.81}$.
          
            \item $\phi_j(x) = \cos(f(x) / j)$ for $j=1, \ldots, 49$. $f(x) = \frac{x}{1.81 \cdot 10^{-1}}$. \footnote{For the trigonometric bases (c) and (d), the periodic nature of cosine requires us to transform the data such that the lengthscale is within the periods of each element of our basis.}
        \end{enumerate}
    
        {\footnotesize *Note: Please make sure to add a bias term for all your basis functions above in your implementation of the \verb|make_basis|.}
    
        Let
        $$ \mathbf{\phi}(\mathbf{X}) = \begin{bmatrix}
            \mathbf{\phi}(x_1) \\
            \mathbf{\phi}(x_2) \\
            \vdots             \\
            \mathbf{\phi}(x_N) \\
        \end{bmatrix} \in \mathbb{R}^{N\times D}. $$
        You will complete the \verb|make_basis| function which must return $\phi(\mathbf{X})$ for each part (a) - (d). You do NOT need to submit this code in your \LaTeX writeup.
    
        Then, create a plot of the fitted regression line for each basis against a scatter plot of the training data. Boilerplate plotting code is provided in the notebook---you will only need to finish up a part of it. \textbf{All you need to include in your writeup for this part are these four plots.}
    
        \item Now we have trained each of our basis regressions. For each basis regression, compute the MSE on the test set. Discuss: do any of the bases seem to overfit? Underfit? Why?
    
        \item Briefly describe what purpose the transforms $\phi$ serve: why are they helpful?
    
        \item As in Problem 1, describe the space and time complexity of linear regression.  How does what is stored to compute predictions change with the size of the training set $N$ and the number of features $D$?  How does the computation needed to compute the prediction for a new input depend on the size of the training set $N$?  How do these complexities compare to those of the kNN and kernelized regressor?
    
        \item Briefly compare and constrast the different regressors: kNN, kernelized regression, and linear regression (with bases). Are some regressions clearly worse than others?  Is there one best regression?  How would you use the fact that you have these multiple regression functions?
    \end{enumerate}
      
    \noindent \textit{Note:} You may be concerned that we are using a different set of inputs $\mathbf{X}$ for each basis (a)-(d), since it could seem as though this prevents us from being able to directly compare the MSE of the models since we are using different data as input. But this is not an issue, since each transformation is considered as being a part of our model. This contrasts with transformations that cause the variance of the target $\mathbf{y}$ to be different  (such as standardization); in these cases the MSE can no longer be directly compared.
\end{problem}

\newpage

\begin{solution}
    \begin{enumerate}
        \item \ \\ \includegraphics[width = \linewidth]{hw1/img_output/p3.1.png}
        \item MSE for part a: 7.956, part b: 8.71, part c: 5.97, part d: 58.91. It is clear from the plot that part (d) severely overfits, which the MSE confirms by being roughly 10x higher than the others. This is mainly because we had 49 terms in our basis, and 57 data points, which makes the curve very sensitive to each point, thus very high variance. Part (a) and (b) have pretty good MSE, but we can see from the plot that they slightly underfit, as the basis can't reflect the periodic nature of the data, so it's too high bias. Part (c) has the best MSE, and also from the plot we can see that it captures the trend of the data really well, because cosine can capture the periodic nature.
        \item The transforms allow us to represent 1D data in multiple dimensions and in terms of non-linear functions. We can then run linear regression in this transformed space, and train a model that can capture the ground truth non-linear relationship in 1D space, essentially lowering bias. It can however also increase variance as we saw in part (d), and so we have to be careful.
        \item Once we train the model for linear regression, to make a prediction for a new $x^*$, all we need to store is the weight vector $w$, so space complexity is $O(D)$, one weight for each feature, since $\hat y = x^* * w$. For kNN and kernelized regression, the complexity was $O(ND)$, so this is faster and doesn't depend on the size of the training set. Time complexity is also $O(D)$ for linear regression, since we only need to compute the dot product between $x^*$ and $w$. For kNN and kernel regression, this was $O(ND)$, and so again this is faster and doesn't depend on $N$.
        \item All regressors have their pros and cons, so there is no best one. kNN and kernel regression are slower and require more space for predictions than linear regression as we discussed. They're however easier to implement and use, since they're non-parametric methods. They're very similar, but the advantage of kernel regression over kNN is that it produces smoother fits and is less influenced by outliers, so it captures general trends better. But if we want purely local averaging, then kNN is better. The downside of linear regression is that its performance heavily depends on choosing the right basis functions, as we've seen. We have to keep these trade-offs in mind and choose the best one for our particular task.
    \end{enumerate}
\end{solution}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Probablistic View of Regression and Regularization, 30pts]
    Finally, we will explore an alternative view of linear regression to what was introduced in lecture. This view will be probabilistic. We will also introduce Bayesian regression under this probabilistic view. We will explore its connection to regularization for linear models, and then fit a regularized model to the temperature data. The probabilistic interpretation of linear regression is explored in more detail in the \href{https://github.com/harvard-ml-courses/cs181-textbook/blob/master/Textbook.pdf}{course notes} under section 2.6.2, but we have also tried to make this question self-contained with all necessary content.
    \\
    
    \noindent Recall that linear regression involves having $N$ labeled data points, say, $(\boldx_n,y_n)$ for $n\in\{1,\dots,N\}$. A probabilistic view of the linear regression problem supposes that the data actually came from a probabilistic model:
    \[y_n = \boldw^\top\boldx_n + \epsilon_n, \quad \epsilon_n \sim \mathcal{N}(0, \sigma^2).\]
    That is, we assume that there exists a set of coefficients $\boldw$ such that given data $\boldx_n$, the corresponding $y_n$ results from taking the $\boldw^\top\boldx_n$ and adding some random noise $\epsilon_n$. Here, we assume the noise is normally distributed with known mean and variance. The introduction of noise into the model accounts for the possibility of scatter, i.e., when the data does not literally follow a perfect line. It is shown in the aforementioned section of the course notes that under this probabilistic model, the data likelihood $p(\boldy|\boldw,\boldX)$ is maximized by $\boldw^* = (\boldX^\top\boldX)^{-1} \boldX^\top \boldy$, which, as we already saw in class, also minimizes the squared error. So, amazingly, the probabilistic view of regression leads to the view we saw in lecture, where we are trying to minimize a prediction error. \\
    
    \noindent Now, Bayesian regression takes this probablistic view a step further. You may recall that Bayesian statistics involves choosing a prior distribution for the parameters, here $\boldw$, based on our prior beliefs. So, in Bayesian regression, we additionally assume the weights are distributed $p(\boldw)$ and fit the weights $\boldw$ by maximizing the posterior likelihood
    \[ p(\boldw | \boldX, \boldy) = \frac{p(\bold y | \boldw, \boldX)p(\boldw)}{p(\boldy | \boldX)}. \]
    Note that since we maximize with respect to $\boldw$, it suffices to just maximize the numerator, while the denominator term does not need to be computed.
    
    \begin{enumerate}
        \item Suppose $\boldw \sim \mathcal{N}(\mathbf{0},\frac{\sigma^2}{\lambda}\boldI)$. Show that maximizing the posterior likelihood is equivalent to minimizing the loss function
        \[\mathcal{L}_{ridge}(\boldw) = \frac{1}{2}||\boldy -\bold X\boldw||_2^2 + \frac{\lambda}{2}||\boldw||_2^2.\] 
        For those who are familiar, note that minimizing $\mathcal{L}_{ridge}(\boldw)$ is exactly what regression with ridge regularization does.
        
        \textit{Hint:} You don't need to explicitly solve for the form of the maximizer/minimizer to show that the optimization problems are equivalent.
        
        \item Solve for the value of $\boldw$ that minimizes $\mathcal L_{ridge}(\boldw)$.
    
        \item The Laplace distribution has the PDF
       \[L(a,b) =\frac{1}{2b} \exp\left(-\frac{|x - a|}{b}\right)\]
        Show that if all $w_d \sim L\left(0,\frac{2\sigma^2}{\lambda}\right)$, maximizing the posterior likelihood is equivalent to minimizing the loss function
        \[\mathcal{L}_{lasso}(\boldw) = \frac{1}{2}||\boldy -\bold X\boldw||_2^2  + \frac{\lambda}{2}||\boldw||_1.\] 
        For those who are familiar, note that minimizing $\mathcal{L}_{lasso}(\boldw)$ is exactly what regression with LASSO regularization does.
    
        \item The LASSO estimator is the value of $\boldw$ that minimizes $\mathcal{L}_{lasso}(\boldw)$? It is very useful in certain real-world scenarios. Why is there no general closed form for the LASSO estimator?
    
        \item Since there is no general closed form for the LASSO estimator $\boldw$, we use numerical methods for estimating $\boldw$. One approach is to use \textit{coordinate descent}, which works as follows: 
        \begin{enumerate}
            \item Initialize $\boldw=\boldw_0$.
            \item For each $d=1, \ldots, D$ do the following 2 steps consecutively:
            \begin{enumerate}
                \item Compute $\rho_d = \tilde{\boldx}_d^\top(\boldy - (\boldX \boldw - w_d \tilde{\boldx}_d))$. We define $\tilde{\boldx}_d$ as the $d$-th column of $\boldX$.
    
                \item If $d=1$, set $w_1 = \frac{\rho_1}{||\tilde{\boldx}_1||^2_2}$. Otherwise if $d\ne 1$, compute $w_d = \frac{\text{sign}(\rho_d)\max\left\{|\rho_d|-\frac{\lambda}{2}, 0\right\}}{||\tilde{\boldx}_d||^2_2}$.
            \end{enumerate}
            \item Repeat step (b) until convergence or the maximum number of iterations is reached.
        \end{enumerate} 
    
        Implement the \texttt{find\_lasso\_weights} function according to the above algorithm, letting $\boldw_0$ be a vector of ones and the max number of iterations be 5000. Then, fit models with $\lambda=1, 10$ to basis (d) from Problem 3 and plot the predictions on the train set. Finally, compute the test MSE's. You will need to do some preprocessing, but a completed helper function for this is already provided. How do the graphs and errors compare to those for the unregularized (i.e., vanilla) basis (d) model? 
    \end{enumerate}
\end{problem}

\newpage

\begin{solution}
	\begin{enumerate}
	    \item As described in the problem, we have \[p(w|X, y) \propto p(y|w, X)p(w).\]
        We know that \[y_n = \boldw^\top\boldx_n + \epsilon_n, \quad \epsilon_n \sim \mathcal{N}(0, \sigma^2),\] which tells us \[p(y_n|w, x_n) \sim N(w^Tx_n, \sigma^2),\] and so \[p(y|w, X) \sim N(Xw, \sigma^2I).\] Using the Normal PDF and the fact that we have $N$ independent observations, \[p(y|w, X) = \frac{1}{(2\pi\sigma^2)^{N/2}} \exp\left(-\frac{1}{2\sigma^2}||y - Xw||^2_2\right).\] Then again using proportionality, \[p(y|w, X) \propto \exp\left(-\frac{1}{2\sigma^2}||y - Xw||^2_2\right).\] We also have \[\boldw \sim \mathcal{N}(\mathbf{0},\frac{\sigma^2}{\lambda}\boldI)\]Using the same method, \[p(w) = \frac{1}{(2\pi \frac{\sigma^2}{\lambda})^{D/2}} \exp\left(-\frac{1}{2} \frac{\lambda}{\sigma^2}  ||w||^2_2\right),\] \[p(w) \propto \exp\left(-\frac{\lambda}{2\sigma^2}  ||w||^2_2\right).\] Combining these together, \[p(w|X, y)\propto \exp\left(-\frac{1}{2\sigma^2}||y - Xw||^2_2\right)\exp\left(-\frac{\lambda}{2\sigma^2}  ||w||^2_2\right),\] \[p(w|X, y) \propto \exp\left(-\frac{1}{2\sigma^2}\left[||y - Xw||^2_2 -\lambda  ||w||^2_2\right]\right).\] Now, we can take the log, \[ \log p(w|X, y) \propto -\frac{1}{2\sigma^2}\left(||y - Xw||^2_2 -\lambda  ||w||^2_2\right).\] Now we note that maximizing the log is the same as minimizing the negative, and we can drop the constant, \[\max_w \log p(w|X, y) = \min_w ||y - Xw||^2_2 +\lambda  ||w||^2_2.\] We therefore finally get \[\max_wp(w|X, y) = \min_w L_{ridge},\] since maximizing the log is the same as maximizing the function, and they only differ by a constant of $1/2$, proving what we wanted.
        \item \[\mathcal{L}_{ridge}(\boldw) = \frac{1}{2}||\boldy -\bold X\boldw||_2^2 + \frac{\lambda}{2}||\boldw||_2^2.\] Taking the gradient with respect to $w$, \[\frac{d}{dw}\mathcal{L}_{ridge} = -X^T(y -Xw) + \lambda w.\] Now, we set the gradient to 0 find the solution, \[0 = -X^T(y -Xw) + \lambda w,\] \[0 = -X^Ty +X^TXw + \lambda w,\] \[X^TXw + \lambda w = X^Ty,\] \[(X^TX + \lambda I)w   = X^Ty,\] \[w  = (X^TX + \lambda I)^{-1} X^Ty.\] We can verify this is a minimum since \[\frac{d^2}{dw^2}\mathcal{L}_{ridge} = X^TX + \lambda I.\] Since for any vector $v$, $||Xv||^2_2 \geq 0$, and $\lambda > 0$, \[\frac{d^2}{dw^2}\mathcal{L}_{ridge} > 0,\]and so this confirms that $w$ is a minimum, which is what we wanted.
        \item The posterior likelihood is \[p(w|X, y) \propto p(y|w, X)p(w).\]
        We showed before that \[p(y|w, X) \propto \exp\left(-\frac{1}{2\sigma^2}||y - Xw||^2_2\right).\] We now have all\[w_d \sim L\left(0,\frac{2\sigma^2}{\lambda}\right).\] Using Lasso PDF and the fact that there are $D$ independent $w_d$, \[p(w) = \left(\frac{\lambda}{4\sigma^2}\right)^D \exp\left(-\frac{\lambda}{2\sigma^2}||w||_1\right).\] Using proportionality, \[p(w) \propto \exp\left(-\frac{\lambda}{2\sigma^2}||w||_1\right).\] Combining these together, \[p(w|X, y)\propto \exp\left(-\frac{1}{2\sigma^2}||y - Xw||^2_2\right)\exp\left(-\frac{\lambda}{2\sigma^2}||w||_1\right),\] \[p(w|X, y) \propto \exp\left(-\frac{1}{2\sigma^2}\left[||y - Xw||^2_2 -\lambda  ||w||_1\right]\right).\] Now, we can take the log, \[ \log p(w|X, y) \propto -\frac{1}{2\sigma^2}\left(||y - Xw||^2_2 -\lambda  ||w||_1\right).\] Now we note that maximizing the log is the same as minimizing the negative, and we can drop the constant, \[\max_w \log p(w|X, y) = \min_w ||y - Xw||^2_2 +\lambda  ||w||_1.\] We therefore finally get \[\max_wp(w|X, y) = \min_w L_{lasso},\] since maximizing the log is the same as maximizing the function, and they only differ by a constant of $1/2$, proving what we wanted.
        \item For $L_{lasso}$, the gradient \[\frac{d}{dw}||w||_1 = \begin{cases} +1 & \text{if } w_d > 0 \\ -1 & \text{if } w_d < 0 \\ \text{undefined} & \text{if } w_d = 0 \end{cases},\] and so it's no longer to find a closed form solution since the gradient depends on the values $w_d$ themselves, unlike in $L_{ridge}$ where we had a linear function in terms of $w$ for the gradient and so we could set it to 0 and solve for any $w$.
        \item MSE for $\lambda = 1: 30.06, \lambda = 10: 15.62$. Both MSE's are better than the non-regularized: $58.91$. The graph also shows that $\lambda = 1$ is pretty similar to the non-regularized version, still extremely overfitting. $\lambda = 10$ looks much better, as it is not as overfitting and captures general pattern better, actually looking closer to part (c) in fit which was the lowest MSE, but still with a significant gap in MSE. It is still incredibly high basis dimension so we would need to regularize more to get close to optimal solution. \\
        \includegraphics[width = \linewidth]{hw1/img_output/p4.5.png}
	\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Name and Calibration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{Name}: Ondrej Vesely

\textbf{Collaborators and Resources}: 

\end{document}
